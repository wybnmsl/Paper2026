# Auto-generated by EoH — accepted GLS solver (framework + evolved heuristic)
# (MODIFIED) Adds dataset loader, tqdm progress, and CSV export for TSPAEL64.pkl
# ---------------------------------------------------------------------------------
# Importable usage (unchanged):
#   from 20251018_155941_seq3_export import GLS_SPEC, build_heuristic_module, solve_instance_export, evaluate_dataset
#   heuristic = build_heuristic_module()
#   gap = solve_instance_export(n, opt_cost, dis_matrix, coord,
#                               time_limit=GLS_SPEC.get('stopping',{}).get('time_limit_s', 2.0),
#                               ite_max=400, perturbation_moves=1)
#
# CLI usage (new):
#   python zTSP/20251018_155941_seq3_export.py \
#       --dataset zTSP/TrainingData/TSPAEL64.pkl \
#       --csv zTSP/evaluation/tspael64_seq3_results.csv \
#       --time_limit 2 --ite_max 400 --perturbation_moves 1
#   # 选项：
#   #   --limit N   只评 N 个实例（默认全量）
#   #   --no_tqdm   关闭进度条
# ---------------------------------------------------------------------------------

import types
import os
import sys

# ===== Robust import for gls.gls_run =====
_here = os.path.abspath(os.path.dirname(__file__))
_candidates = [
    _here,                                   # zTSP/ (if script placed here)
    os.path.abspath(os.path.join(_here, ".")),
    os.path.abspath(os.path.join(_here, "..")),   # project root
]
for _p in _candidates:
    if _p not in sys.path:
        sys.path.insert(0, _p)

try:
    from gls.gls_run import solve_instance_with_spec
except Exception:
    try:
        from zTSP.gls.gls_run import solve_instance_with_spec
    except Exception as _e:
        raise ImportError(
            "Cannot import solve_instance_with_spec. Tried 'gls.gls_run' and 'zTSP.gls.gls_run'. "
            "Run from repo root or zTSP/, or add project root/zTSP to PYTHONPATH."
        ) from _e

# ===== Accepted GLS spec from seq3 (保持原样) =====
GLS_SPEC = {
  "init": {"method": "nearest_neighbor", "start": 0},
  "candset": {"type": "kNN", "k": 40},
  "operators": [
    {"name": "two_opt", "strategy": "first"},
    {"name": "relocate", "strategy": "first"}
  ],
  "schedule": {"loop_max": 400, "max_no_improve": 50},
  "accept": {"type": "improve_only", "temp0": 0.0},
  "perturb": {"type": "double_bridge", "interval": 40},
  "guidance": {"where": "mid_ls", "weight": 0.9, "top_k": 3},
  "stopping": {"time_limit_s": 2.0}
}

def build_heuristic_module():
    """Build a Python module containing the evolved `update_edge_distance` function."""
    USER_CODE = r'''
def update_edge_distance(edge_distance, local_opt_tour, edge_n_used):
    import numpy as np
    rng = np.random.default_rng()

    # Defensive copies and checks
    ed = np.array(edge_distance, dtype=float, copy=True)
    used = np.array(edge_n_used, dtype=float, copy=True)
    tour_in = np.asarray(local_opt_tour).astype(int).flatten().copy()
    eps = 1e-12

    if ed.ndim != 2 or ed.shape[0] != ed.shape[1]:
        raise ValueError("edge_distance must be an NxN matrix")
    N = ed.shape[0]
    if used.shape != (N, N):
        raise ValueError("edge_n_used must be an NxN matrix")

    # If no tour provided, return simple symmetrized base distances
    if tour_in.size == 0:
        updated_edge_distance = 0.5 * (ed + ed.T)
        np.fill_diagonal(updated_edge_distance, 0.0)
        return updated_edge_distance

    # Accept 1- or 0-based indices
    if tour_in.max() == N:
        tour_in = tour_in - 1
    if tour_in.max() > N - 1 or tour_in.min() < 0:
        raise ValueError("local_opt_tour contains invalid node indices")

    # Base symmetric length and usage
    length = 0.5 * (ed + ed.T)
    usage = 0.5 * (used + used.T)

    # Prepare upper-triangle edge vectorization
    iu, ju = np.triu_indices(N, k=1)
    m = iu.size
    if m == 0:
        updated_edge_distance = length.copy()
        return updated_edge_distance

    length_vec = length[iu, ju]
    usage_vec = usage[iu, ju]
    max_usage = max(1.0, np.max(usage_vec))
    usage_norm_vec = usage_vec / max_usage

    # Build tour mask (full matrix) and vector form
    tour_mask = np.zeros((N, N), dtype=float)
    j = np.roll(tour_in, -1)
    tour_mask[tour_in, j] = 1.0
    tour_mask[j, tour_in] = 1.0
    tour_mask_vec = tour_mask[iu, ju]

    mean_len = float(np.mean(length) if length.size else 1.0)

    # === 1) Diffusion of "heat" seeded on tour + usage counts ===
    h0 = 1.3 * tour_mask_vec + 0.6 * usage_norm_vec
    if h0.size:
        h0 = h0 / (h0.max() + eps)

    # Precompute node degrees
    deg_nodes = np.zeros(N, dtype=float)
    np.add.at(deg_nodes, iu, 1.0)
    np.add.at(deg_nodes, ju, 1.0)
    deg_nodes = np.maximum(deg_nodes, 1.0)

    h = h0.copy()
    alpha = 0.78
    K = 6
    for _ in range(K):
        node_heat = np.zeros(N, dtype=float)
        np.add.at(node_heat, iu, h)
        np.add.at(node_heat, ju, h)
        new_h = (node_heat[iu] / deg_nodes[iu] + node_heat[ju] / deg_nodes[ju]) * 0.5
        h = (1.0 - alpha) * h0 + alpha * new_h
        h = np.clip(h, 0.0, 2.0)

    heat_vec = h
    if heat_vec.max() > eps:
        heat_scaled = heat_vec / (heat_vec.max() + eps)
    else:
        heat_scaled = heat_vec

    # === 2) Explicit 2-opt improvement estimates per candidate edge ===
    tour_nodes = np.asarray(tour_in, dtype=int)
    pos = -np.ones(N, dtype=int)
    for idx, node in enumerate(tour_nodes):
        pos[node] = idx

    tour_len = tour_nodes.size
    next_node = np.empty(N, dtype=int)
    prev_node = np.empty(N, dtype=int)
    for node in range(N):
        p = pos[node]
        if p == -1:
            next_node[node] = node
            prev_node[node] = node
        else:
            next_node[node] = tour_nodes[(p + 1) % tour_len]
            prev_node[node] = tour_nodes[(p - 1) % tour_len]

    i_arr = iu
    j_arr = ju
    a1 = next_node[i_arr]; a2 = prev_node[i_arr]
    b1 = next_node[j_arr]; b2 = prev_node[j_arr]

    d_ij = length[i_arr, j_arr]
    d_i_a1 = length[i_arr, a1]
    d_i_a2 = length[i_arr, a2]
    d_j_b1 = length[j_arr, b1]
    d_j_b2 = length[j_arr, b2]
    d_a1_b1 = length[a1, b1]
    d_a1_b2 = length[a1, b2]
    d_a2_b1 = length[a2, b1]
    d_a2_b2 = length[a2, b2]

    delta1 = d_ij + d_a1_b1 - d_i_a1 - d_j_b1
    delta2 = d_ij + d_a1_b2 - d_i_a1 - d_j_b2
    delta3 = d_ij + d_a2_b1 - d_i_a2 - d_j_b1
    delta4 = d_ij + d_a2_b2 - d_i_a2 - d_j_b2

    delta_min = np.minimum(np.minimum(delta1, delta2), np.minimum(delta3, delta4))
    attraction = np.maximum(0.0, -delta_min)
    max_attr = attraction.max()
    if max_attr > eps:
        attraction_scaled = attraction / (max_attr + eps)
    else:
        attraction_scaled = attraction * 0.0

    # === 3) Compose update factors ===
    gamma_heat = 1.25
    beta_usage = 1.10
    usage_power = 1.6
    eta_attract = 0.82
    tour_edge_boost = 1.8

    heat_pen = 1.0 + gamma_heat * heat_scaled
    usage_pen = 1.0 + beta_usage * (usage_norm_vec ** usage_power)
    tour_pen = 1.0 + tour_edge_boost * tour_mask_vec

    multi_factor = heat_pen * usage_pen * tour_pen
    attract_discount = 1.0 - eta_attract * attraction_scaled
    attract_discount = np.clip(attract_discount, 0.30, 1.0)

    updated_vec = length_vec * multi_factor * attract_discount

    # === 4) Additive exploratory perturbations ===
    jitter_scale = 0.015 * mean_len
    jitter = rng.normal(loc=0.0, scale=jitter_scale, size=m)
    updated_vec = updated_vec + jitter

    if rng.random() < 0.14:
        frac = 0.045
        k = max(1, int(np.ceil(frac * m)))
        chosen = rng.choice(m, size=k, replace=False)
        percs = rng.random(size=k) * 100.0
        vals = np.percentile(length_vec, percs)
        updated_vec[chosen] = vals * (1.0 + rng.normal(scale=0.06, size=k))

    updated_vec = np.maximum(updated_vec, 1e-12)

    updated = np.zeros((N, N), dtype=float)
    updated[iu, ju] = updated_vec
    updated[ju, iu] = updated_vec
    np.fill_diagonal(updated, 0.0)

    mean_before = np.mean(length) if length.size else 1.0
    mean_after = np.mean(updated) if updated.size else 1.0
    if mean_after > eps:
        scale = mean_before / mean_after
        scale = float(np.clip(scale, 0.6, 1.7))
        updated *= scale

    updated = 0.5 * (updated + updated.T)
    updated = np.maximum(updated, 1e-12)
    np.fill_diagonal(updated, 0.0)

    return updated
'''
    m = types.ModuleType("heuristic_module")
    exec(USER_CODE, m.__dict__)
    return m

def solve_instance_export(n, opt_cost, dis_matrix, coord,
                          time_limit=2.0, ite_max=400, perturbation_moves=1):
    """Solve a single instance using the exported heuristic + GLS_SPEC.
    Return value is gap percent (consistent with your main runner)."""
    heuristic = build_heuristic_module()
    return solve_instance_with_spec(
        n, float(opt_cost), dis_matrix, coord,
        time_limit, ite_max, perturbation_moves, heuristic, GLS_SPEC
    )

def evaluate_dataset(coords_list, instances_list, opt_costs,
                     time_limit=2.0, ite_max=400, perturbation_moves=1):
    """Evaluate a list of instances; returns average gap (percent)."""
    import numpy as _np
    gaps = []
    for i in range(len(instances_list)):
        gap = solve_instance_export(i, float(opt_costs[i]), instances_list[i], coords_list[i],
                                    time_limit, ite_max, perturbation_moves)
        if gap < 0 and abs(gap) < 1e-9:
            gap = 0.0
        gaps.append(float(gap))
    return float(_np.mean(_np.array(gaps, dtype=_np.float64)))

# =========================
# Dataset helpers & CLI
# =========================

def load_tspael64(pkl_path):
    """Load zTSP/TrainingData/TSPAEL64.pkl (dict with lists of arrays)."""
    import pickle
    import numpy as np
    with open(pkl_path, "rb") as f:
        data = pickle.load(f)
    coords = data.get("coordinate")
    dists  = data.get("distance_matrix")
    costs  = data.get("cost")
    if coords is None or dists is None or costs is None:
        raise ValueError("TSPAEL64.pkl missing required keys: coordinate / distance_matrix / cost")
    coords = [np.asarray(c) for c in coords]
    dists  = [np.asarray(D) for D in dists]
    costs  = [float(x) for x in costs]
    if not (len(coords) == len(dists) == len(costs)):
        raise ValueError("coordinate / distance_matrix / cost length mismatch")
    return coords, dists, costs

def evaluate_tspael64_to_csv(pkl_path, csv_out,
                             time_limit=None, ite_max=400, perturbation_moves=1,
                             limit=None, tqdm_disable=False):
    """Run full-batch evaluation on TSPAEL64.pkl and write a CSV summary."""
    import time as _time
    import numpy as _np
    try:
        import pandas as _pd
    except Exception:
        _pd = None
    from tqdm import tqdm

    coords, dists, costs = load_tspael64(pkl_path)
    n_all = len(dists)
    n = min(n_all, int(limit)) if (limit is not None) else n_all

    if time_limit is None:
        time_limit = float(GLS_SPEC.get("stopping", {}).get("time_limit_s", 2.0))

    rows = []
    it = tqdm(range(n), disable=tqdm_disable, desc="Evaluating TSPAEL64 (seq3)", unit="inst")

    for i in it:
        opt = float(costs[i])
        D   = dists[i]
        C   = coords[i]
        N   = int(D.shape[0]) if hasattr(D, "shape") and D.ndim >= 2 else None

        t0 = _time.time()
        try:
            gap = solve_instance_export(i, opt, D, C, time_limit=time_limit,
                                        ite_max=ite_max, perturbation_moves=perturbation_moves)
            if gap < 0 and abs(gap) < 1e-9:
                gap = 0.0
            err_msg = ""
        except Exception as e:
            gap = float("nan")
            err_msg = str(e)
        dt = _time.time() - t0

        sol_est = opt * (1.0 + (gap/100.0)) if _np.isfinite(gap) else float("nan")

        rows.append({
            "idx": i,
            "N": N,
            "opt_cost": opt,
            "sol_cost_est": sol_est,
            "gap_percent": gap,
            "time_s": dt,
            "time_limit_s": time_limit,
            "loop_max": GLS_SPEC.get("schedule", {}).get("loop_max"),
            "max_no_improve": GLS_SPEC.get("schedule", {}).get("max_no_improve"),
            "k": GLS_SPEC.get("candset", {}).get("k"),
            "top_k": GLS_SPEC.get("guidance", {}).get("top_k"),
            "perturb": GLS_SPEC.get("perturb", {}).get("type"),
            "perturb_interval": GLS_SPEC.get("perturb", {}).get("interval"),
            "error": err_msg,
        })

    # write CSV
    if _pd is not None:
        import os as _os
        _os.makedirs(os.path.dirname(csv_out) or ".", exist_ok=True)
        df = _pd.DataFrame(rows)
        df.to_csv(csv_out, index=False)
    else:
        import csv as _csv
        import os as _os
        _os.makedirs(os.path.dirname(csv_out) or ".", exist_ok=True)
        keys = list(rows[0].keys()) if rows else []
        with open(csv_out, "w", newline="", encoding="utf-8") as f:
            w = _csv.DictWriter(f, fieldnames=keys)
            w.writeheader()
            w.writerows(rows)

    avg_gap = float(_np.nanmean([r["gap_percent"] for r in rows])) if rows else float("nan")
    avg_time = float(_np.nanmean([r["time_s"] for r in rows])) if rows else float("nan")
    print(f"[summary] instances={n}/{n_all} | avg_gap%={avg_gap:.6f} | avg_time_s={avg_time:.3f} | csv -> {csv_out}")

    return rows

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Evaluate accepted GLS export (seq3) on TSPAEL64.pkl and write CSV (with tqdm).")
    parser.add_argument("--dataset", type=str, default="zTSP/TrainingData/TSPAEL64.pkl",
                        help="Path to TSPAEL64.pkl (default: zTSP/TrainingData/TSPAEL64.pkl)")
    parser.add_argument("--csv", type=str, default="zTSP/evaluation/tspael64_seq3_results.csv",
                        help="Output CSV path (default: zTSP/evaluation/tspael64_seq3_results.csv)")
    parser.add_argument("--time_limit", type=float, default=None,
                        help="Override time_limit (sec); default uses GLS_SPEC.stopping.time_limit_s=2.0")
    parser.add_argument("--ite_max", type=int, default=400, help="ite_max passed to solver (default: 400, per spec)")
    parser.add_argument("--perturbation_moves", type=int, default=1, help="perturbation_moves (default: 1)")
    parser.add_argument("--limit", type=int, default=None, help="Evaluate only first N instances (default: all)")
    parser.add_argument("--no_tqdm", action="store_true", help="Disable tqdm progress bar")
    args = parser.parse_args()

    # ensure parent dirs for CSV
    os.makedirs(os.path.dirname(args.csv) or ".", exist_ok=True)

    evaluate_tspael64_to_csv(
        pkl_path=args.dataset,
        csv_out=args.csv,
        time_limit=args.time_limit,
        ite_max=args.ite_max,
        perturbation_moves=args.perturbation_moves,
        limit=args.limit,
        tqdm_disable=args.no_tqdm
    )
